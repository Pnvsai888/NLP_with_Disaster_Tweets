{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVVU4qZg3hLd",
        "outputId": "4c578f57-693a-4e56-88b1-28734cb7eeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy nltk scikit-learn gensim tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfqUKjLr6R3_",
        "outputId": "2f05e2e6-5e38-47c1-d932-1d3a0ee10332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFQd48qHH0QI"
      },
      "source": [
        "Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07bI7c9F8fH9",
        "outputId": "47fd68b3-c7e8-4a7c-b8e3-8e9167fe00a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id keyword        location  \\\n",
            "0   1  ablaze             NaN   \n",
            "1   2  ablaze             NaN   \n",
            "2   3  ablaze   New York City   \n",
            "3   4  ablaze  Morgantown, WV   \n",
            "4   5  ablaze             NaN   \n",
            "\n",
            "                                                text  target  \n",
            "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
            "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
            "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "4  \"Lord Jesus, your love brings freedom and pard...       0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "file_path = 'tweet.csv'  # Replace with your file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he7RDLYSH9Fg"
      },
      "source": [
        "Pre-processing and Cleaning of Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-L3TXns8wgk",
        "outputId": "d0e78bef-956e-43d8-f909-c808cc5102e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset saved as cleaned_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download stopwords and wordnet data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load your dataset\n",
        "file_path = 'tweet.csv'  # Replace with your file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase the text\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'\\@w+|\\#','', text)  # Remove mentions and hashtags\n",
        "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)  # Remove special characters\n",
        "    text = text.strip()  # Remove leading/trailing whitespace\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])  # Lemmatize words\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "df['clean_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Remove the original 'text' column\n",
        "df.drop(columns=['text'], inplace=True)\n",
        "\n",
        "# Save cleaned dataset to a new CSV file with the specified name\n",
        "cleaned_file_name = 'cleaned_dataset.csv'\n",
        "df.to_csv(cleaned_file_name, index=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved as {cleaned_file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMqU_E2z-OHb",
        "outputId": "4ee54a25-9283-4580-b3cb-abac4c024a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id  keyword        location  target  \\\n",
            "0        1   ablaze             NaN       1   \n",
            "1        2   ablaze             NaN       1   \n",
            "2        3   ablaze   New York City       1   \n",
            "3        4   ablaze  Morgantown, WV       1   \n",
            "4        5   ablaze             NaN       0   \n",
            "...    ...      ...             ...     ...   \n",
            "1995  1996  wounded             NaN       1   \n",
            "1996  1997  wounded             NaN       1   \n",
            "1997  1998  wounded             NaN       0   \n",
            "1998  1999  wounded             NaN       1   \n",
            "1999  2000  wounded             NaN       1   \n",
            "\n",
            "                                             clean_text  \n",
            "0     communal violence bhainsa telangana stone pelt...  \n",
            "1     telangana section 144 imposed bhainsa january ...  \n",
            "2                    arsonist set car ablaze dealership  \n",
            "3                    arsonist set car ablaze dealership  \n",
            "4     lord jesus love brings freedom pardon fill hol...  \n",
            "...                                                 ...  \n",
            "1995  almost 1500 innocent iranian got shot street t...  \n",
            "1996  twenty one saudi military cadet expelled one c...  \n",
            "1997                                  wounded persevere  \n",
            "1998  iraqi center war crime documentation 669 marty...  \n",
            "1999  police iran denied using live ammunition dispe...  \n",
            "\n",
            "[2000 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df.head(2000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ0QukBcIIdb"
      },
      "source": [
        "Convert Text Data into Numerical Representations using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAYS6m8E_3hN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Load the cleaned dataset\n",
        "cleaned_file_path = 'cleaned_dataset.csv'\n",
        "clean_df = pd.read_csv(cleaned_file_path)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(clean_df['clean_text'])\n",
        "\n",
        "# Save the TF-IDF features\n",
        "with open('tfidf_features.pkl', 'wb') as file:\n",
        "    pickle.dump(tfidf_vectorizer, file)\n",
        "\n",
        "# Assuming 'target' is the column with labels\n",
        "# Replace 'target' with the actual column name if different\n",
        "y = clean_df['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2vJR5JPAeE3",
        "outputId": "615a153e-7c12-491a-d799-9fe8d277fb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Features:\n",
            "  (0, 82)\t0.265232073123263\n",
            "  (0, 3462)\t0.2364878885772171\n",
            "  (0, 4633)\t0.2543520651716344\n",
            "  (0, 1714)\t0.44808564694705205\n",
            "  (0, 2150)\t0.265232073123263\n",
            "  (0, 2463)\t0.30322981823930834\n",
            "  (0, 3881)\t0.2744856336932624\n",
            "  (0, 4135)\t0.28641554816606285\n",
            "  (0, 314)\t0.30322981823930834\n",
            "  (0, 4674)\t0.293976257669309\n",
            "  (0, 767)\t0.3319740027853543\n",
            "  (1, 2607)\t0.2994662462713562\n",
            "  (1, 9)\t0.23243951450258682\n",
            "  (1, 1605)\t0.22667299480716893\n",
            "  (1, 4473)\t0.19239608757586613\n",
            "  (1, 1330)\t0.26518933904005343\n",
            "  (1, 665)\t0.27353676692685114\n",
            "  (1, 13)\t0.252602474151674\n",
            "  (1, 11)\t0.24320122076733267\n",
            "  (1, 1825)\t0.443355616475682\n",
            "  (1, 1754)\t0.28429847319159696\n",
            "  (1, 3399)\t0.28429847319159696\n",
            "  (1, 4135)\t0.2583689938470919\n",
            "  (1, 314)\t0.27353676692685114\n",
            "  (2, 1043)\t0.5245743477055231\n",
            "  :\t:\n",
            "  (2201, 819)\t0.31996582234107573\n",
            "  (2201, 1489)\t0.2907832977632908\n",
            "  (2201, 2151)\t0.2380401183295643\n",
            "  (2201, 4578)\t0.31996582234107573\n",
            "  (2201, 4432)\t0.2190108741050266\n",
            "  (2201, 4079)\t0.2230235740663911\n",
            "  (2201, 2403)\t0.24009425662076878\n",
            "  (2202, 4968)\t0.3315871179871933\n",
            "  (2202, 4918)\t0.2510525414217671\n",
            "  (2202, 1359)\t0.31903536867636634\n",
            "  (2202, 4944)\t0.30134467162358397\n",
            "  (2202, 2091)\t0.2585504759491477\n",
            "  (2202, 2211)\t0.2676099117059334\n",
            "  (2202, 2596)\t0.30929947553379633\n",
            "  (2202, 1556)\t0.2585504759491477\n",
            "  (2202, 218)\t0.288792922312757\n",
            "  (2202, 2140)\t0.23413408802242464\n",
            "  (2202, 4440)\t0.30929947553379633\n",
            "  (2202, 4212)\t0.21153590598482172\n",
            "  (2202, 747)\t0.23413408802242464\n",
            "  (2203, 3712)\t0.5221891752933402\n",
            "  (2203, 4918)\t0.39536191983608043\n",
            "  (2203, 111)\t0.487090207327939\n",
            "  (2203, 317)\t0.46397110815795506\n",
            "  (2203, 4288)\t0.34421121171439867\n"
          ]
        }
      ],
      "source": [
        "# Display TF-IDF features\n",
        "print(\"TF-IDF Features:\")\n",
        "print(X_tfidf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K1NkahmIP8j"
      },
      "source": [
        " Explore and Integrate Word Embedding Techniques using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3-8oGmYAgyc"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Prepare the dataset for Word2Vec\n",
        "sentences = [row.split() for row in clean_df['clean_text']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the Word2Vec model\n",
        "word2vec_model.save(\"word2vec_model.model\")\n",
        "\n",
        "# Create Word2Vec feature vectors\n",
        "def get_word2vec_vector(text):\n",
        "    words = text.split()\n",
        "    vector = np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv], axis=0)\n",
        "    return vector\n",
        "\n",
        "X_word2vec = np.array([get_word2vec_vector(text) for text in clean_df['clean_text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUWGenBbAvwI",
        "outputId": "56b87151-b266-4aef-8299-d9fecc5c8e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Model Information:\n",
            "Vocabulary Size: 7354\n",
            "Word Vector Dimensionality: 100\n",
            "Most similar words to 'disaster': [('foreign', 0.3486267924308777), ('5am', 0.3434354364871979), ('forc', 0.3370402753353119), ('csgogiveaway', 0.3301014006137848), ('grasp', 0.3298601508140564), ('bruh', 0.32777780294418335), ('fascination', 0.3248547911643982), ('overtopping', 0.3226398527622223), ('may', 0.32150524854660034), ('become', 0.31580862402915955)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the dataset for Word2Vec\n",
        "sentences = [row.split() for row in clean_df['clean_text']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Print some information about the Word2Vec model\n",
        "print(\"Word2Vec Model Information:\")\n",
        "print(\"Vocabulary Size:\", len(word2vec_model.wv))\n",
        "print(\"Word Vector Dimensionality:\", word2vec_model.wv.vector_size)\n",
        "print(\"Most similar words to 'disaster':\", word2vec_model.wv.most_similar('disaster'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QESAAdCIIUio"
      },
      "source": [
        "Select and Compare Machine Learning Algorithms\n",
        "python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aHFotUMDEoh",
        "outputId": "4505ac03-e1b3-4cd0-9126-63240588ae29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Test Accuracy: 0.8027\n",
            "Random Forest Test Accuracy: 0.8322\n",
            "Support Vector Machine Test Accuracy: 0.8186\n",
            "Multinomial Naive Bayes Test Accuracy: 0.8095\n",
            "\n",
            "Best Classifier: Random Forest with Accuracy: 0.8322\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Support Vector Machine': SVC(),\n",
        "    'Multinomial Naive Bayes': MultinomialNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "best_classifier = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    # Train the classifier\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Evaluate the classifier\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{clf_name} Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Update best classifier if necessary\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_classifier = clf_name\n",
        "\n",
        "print(f\"\\nBest Classifier: {best_classifier} with Accuracy: {best_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndIik_8ETcvB"
      },
      "source": [
        "Training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBg2nD6pG9e1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "data = pd.read_csv('cleaned_dataset.csv')  # Ensure this CSV has clean_text and target columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5eOFnimG9e1"
      },
      "outputs": [],
      "source": [
        "# Separate features and target variable\n",
        "X = data['clean_text']  # Assuming 'clean_text' column contains preprocessed tweets\n",
        "y = data['target']      # Assuming 'target' column contains the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8xSFq9HG9e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Split the data into training and temporary set (80% train, 20% temp)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNLS-LnFG9e2"
      },
      "outputs": [],
      "source": [
        "# Split the temporary set into validation and testing sets (50% val, 50% test)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gaovdGBG9e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP-9OPo-G9e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EjibHygG9e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Transform the validation and test data\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itlIHykxG9e2"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "rf_model = RandomForestClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "fkBhAb3iG9e2",
        "outputId": "dbb90ec2-994d-4c30-d81f-7f2493d77b0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Train the model\n",
        "rf_model.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80GLNqHxG9e2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10VDfd7MG9e2"
      },
      "outputs": [],
      "source": [
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "HtIi4l49G9e2",
        "outputId": "b1cb7e49-4c0e-4590-9d74-411f27379ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
              "             param_grid={'max_depth': [None, 10, 20, 30],\n",
              "                         'min_samples_leaf': [1, 2, 4],\n",
              "                         'min_samples_split': [2, 5, 10],\n",
              "                         'n_estimators': [50, 100, 200]},\n",
              "             scoring='accuracy', verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
              "             param_grid={&#x27;max_depth&#x27;: [None, 10, 20, 30],\n",
              "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
              "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
              "             param_grid={&#x27;max_depth&#x27;: [None, 10, 20, 30],\n",
              "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
              "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
              "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train_tfidf, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FPX6ze9G9e2",
        "outputId": "c680370c-b6e8-470b-effd-fd95331d871f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f'Best parameters: {best_params}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIVs0ZWOG9e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train the model with the best parameters\n",
        "best_rf_model = grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBz8x4IOG9e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = best_rf_model.predict(X_val_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPHT4sn2G9e3"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "val_recall = recall_score(y_val, y_val_pred, average='weighted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqoIOpU6G9e3",
        "outputId": "8428aff3-7ac2-422f-fda8-6ff4f2dbfe89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8318181818181818\n",
            "Validation Precision: 0.8272045454545455\n",
            "Validation Recall: 0.8318181818181818\n"
          ]
        }
      ],
      "source": [
        "print(f'Validation Accuracy: {val_accuracy}')\n",
        "print(f'Validation Precision: {val_precision}')\n",
        "print(f'Validation Recall: {val_recall}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riSoE60G9e3",
        "outputId": "8232fbf8-f1e6-481c-bf7e-4812ea4a350c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.98      0.90       171\n",
            "           1       0.80      0.33      0.46        49\n",
            "\n",
            "    accuracy                           0.83       220\n",
            "   macro avg       0.82      0.65      0.68       220\n",
            "weighted avg       0.83      0.83      0.80       220\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Detailed classification report\n",
        "print(classification_report(y_val, y_val_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDBjIfKlG9e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = best_rf_model.predict(X_test_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY1CferCG9e3",
        "outputId": "bb6a568f-d5a3-45bc-811f-963a159e3e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8280542986425339\n",
            "Test Precision: 0.8187933634992458\n",
            "Test Recall: 0.8280542986425339\n"
          ]
        }
      ],
      "source": [
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "print(f'Test Precision: {test_precision}')\n",
        "print(f'Test Recall: {test_recall}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhcvxxpXG9e4",
        "outputId": "00991beb-a699-4d54-b332-0268a874c818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.97      0.90       172\n",
            "           1       0.76      0.33      0.46        49\n",
            "\n",
            "    accuracy                           0.83       221\n",
            "   macro avg       0.80      0.65      0.68       221\n",
            "weighted avg       0.82      0.83      0.80       221\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Detailed classification report\n",
        "print(classification_report(y_test, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cWFJ9bOG9e4"
      },
      "outputs": [],
      "source": [
        "# Function to classify new tweets\n",
        "def classify_tweet(tweet):\n",
        "    # Assuming the tweet is already preprocessed\n",
        "    tweet_tfidf = tfidf_vectorizer.transform([tweet])\n",
        "\n",
        "    # Predict using the trained model\n",
        "    prediction = best_rf_model.predict(tweet_tfidf)\n",
        "\n",
        "    return 'Disaster' if prediction == 1 else 'Not Disaster'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCHfSbgVG9e5",
        "outputId": "8091c110-afcc-456e-a49d-44989a9ad793"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVptgI91G9e9",
        "outputId": "a5489477-e749-4ed3-d17c-58c04a8abcb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['random_forest_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "joblib.dump(rf_model, 'random_forest_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4cNcr7SPniq",
        "outputId": "bc1322ea-b63d-437a-a5ab-a40d7dfd5575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.1)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.36.0 watchdog-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import joblib\n",
        "\n",
        "# Load the trained model\n",
        "model = joblib.load('tfidf_features.pkl')\n",
        "\n",
        "# Set up the Streamlit UI\n",
        "st.title(\"Text Classification App\")\n",
        "st.write(\"Enter some text and click the button to classify it.\")\n",
        "\n",
        "# Create a text input box\n",
        "text_input = st.text_area(\"Enter text here:\")\n",
        "\n",
        "# Create a button to classify the text\n",
        "if st.button(\"Classify\"):\n",
        "    # Perform the classification (replace with your actual logic)\n",
        "    prediction = model.predict([text_input])\n",
        "    confidence = model.predict_proba([text_input])\n",
        "\n",
        "    # Display the result\n",
        "    label = 'positive' if prediction[0] == 1 else 'negative'\n",
        "    confidence_score = max(confidence[0])\n",
        "    st.write(f\"Classification: {label}\")\n",
        "    st.write(f\"Confidence: {confidence_score:.2f}\")"
      ],
      "metadata": {
        "id": "05nMHo-xMPQo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}